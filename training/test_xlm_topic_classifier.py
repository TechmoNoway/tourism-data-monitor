"""
Test script for trained XLM-RoBERTa Topic Classifier
Loads the trained model and tests on multilingual sample comments.

Usage:
    python training/test_xlm_topic_classifier.py
    python training/test_xlm_topic_classifier.py --model_path training/models/xlm_best_model.pt
"""

import os
import sys
import argparse
import torch
from transformers import AutoTokenizer

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training.train_xlm_tourism_topic_classifier import XLMRoBERTaTopicClassifier, TOPICS


def load_model(model_path: str, device: str = 'cuda'):
    """Load trained model from checkpoint"""
    print(f"üì¶ Loading model from {model_path}...")
    
    checkpoint = torch.load(model_path, map_location=device)
    
    model = XLMRoBERTaTopicClassifier(n_classes=len(TOPICS))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"‚úÖ Model loaded successfully!")
    print(f"   Trained epoch: {checkpoint['epoch']}")
    print(f"   F1 Macro: {checkpoint['f1_macro']:.4f}")
    print(f"   Threshold: {checkpoint['threshold']}")
    
    return model, checkpoint['threshold']


def predict(model, tokenizer, text: str, threshold: float, device: str, max_length: int = 256):
    """Predict topics for a single comment"""
    # Tokenize
    encoding = tokenizer(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    # Predict
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits)
    
    # Get predictions above threshold
    probs = probs.cpu().numpy()[0]
    predictions = []
    
    for idx, (topic, prob) in enumerate(zip(TOPICS, probs)):
        if prob >= threshold:
            predictions.append({
                'topic': topic,
                'confidence': float(prob)
            })
    
    # Sort by confidence
    predictions = sorted(predictions, key=lambda x: x['confidence'], reverse=True)
    
    return predictions, probs


def main(args):
    """Main testing function"""
    print("=" * 80)
    print("üß™ XLM-RoBERTa Topic Classifier - Testing")
    print("=" * 80)
    
    # Set device
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"\nüîß Using device: {device}")
    
    # Load model
    model, threshold = load_model(args.model_path, device)
    
    # Load tokenizer
    print("\nüìù Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
    
    # Test samples (multilingual)
    test_samples = [
        # English
        "Amazing scenery! The mountain view is breathtaking and the sunset is gorgeous.",
        "The food was delicious, especially the fresh seafood dishes.",
        "Staff was rude and unprofessional. Very disappointed with the service.",
        "Too expensive for what you get. Not worth the price at all.",
        
        # Korean
        "Í≤ΩÏπòÍ∞Ä Ï†ïÎßê ÏïÑÎ¶ÑÎãµÎÑ§Ïöî! ÏÇ¨ÏßÑ Ï∞çÍ∏∞ Ï¢ãÏùÄ Í≥≥ÏûÖÎãàÎã§.",
        "ÏùåÏãùÏù¥ ÎßõÏûàÏñ¥Ïöî. ÌäπÌûà ÌòÑÏßÄ ÏöîÎ¶¨Í∞Ä ÌõåÎ•≠Ìï©ÎãàÎã§.",
        "ÏßÅÏõêÎì§Ïù¥ ÏπúÏ†àÌïòÍ≥† ÏãúÏÑ§Ïù¥ Íπ®ÎÅóÌï©ÎãàÎã§.",
        
        # Chinese
        "È£éÊôØÂ§™Áæé‰∫ÜÔºÅÊãçÁÖßË∂ÖÁ∫ßÂ•ΩÁúãÁöÑÂú∞Êñπ„ÄÇ",
        "È£üÁâ©ÂæàÂ•ΩÂêÉÔºåÊµ∑È≤úÂæàÊñ∞È≤ú„ÄÇ",
        "ÊúçÂä°ÊÄÅÂ∫¶‰∏çÂ•ΩÔºåÊàøÈó¥‰πü‰∏çÂπ≤ÂáÄ„ÄÇ",
        
        # Japanese
        "ÊôØËâ≤„ÅåÁ¥†Êô¥„Çâ„Åó„ÅÑÔºÅÂÜôÁúüÊò†„Åà„Åô„ÇãÂ†¥ÊâÄ„Åß„Åô„ÄÇ",
        "ÊñôÁêÜ„ÅåÁæéÂë≥„Åó„ÅÑ„ÄÅÁâπ„Å´Êµ∑ÈÆÆÊñôÁêÜ„ÅåÊñ∞ÈÆÆ„Åß„Åô„ÄÇ",
        "„Çπ„Çø„ÉÉ„Éï„ÅåË¶™Âàá„Åß„ÄÅÊñΩË®≠„Åå„Åç„Çå„ÅÑ„Åß„Åô„ÄÇ",
        
        # Russian
        "–ö—Ä–∞—Å–∏–≤—ã–µ –≤–∏–¥—ã! –û—Ç–ª–∏—á–Ω–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π.",
        "–ï–¥–∞ –≤–∫—É—Å–Ω–∞—è, –æ—Å–æ–±–µ–Ω–Ω–æ –º–æ—Ä–µ–ø—Ä–æ–¥—É–∫—Ç—ã —Å–≤–µ–∂–∏–µ.",
        "–ü–µ—Ä—Å–æ–Ω–∞–ª –≥—Ä—É–±—ã–π, –∫–æ–º–Ω–∞—Ç—ã –≥—Ä—è–∑–Ω—ã–µ.",
    ]
    
    print("\n" + "=" * 80)
    print("üéØ Testing on Multilingual Sample Comments")
    print("=" * 80)
    
    for idx, text in enumerate(test_samples, 1):
        print(f"\nüìù Comment {idx}:")
        print(f"   \"{text}\"")
        
        predictions, all_probs = predict(model, tokenizer, text, threshold, device)
        
        if predictions:
            print(f"\n   ‚úÖ Predicted Topics:")
            for pred in predictions:
                print(f"      {pred['topic']:12s}: {pred['confidence']:.2%}")
        else:
            print(f"\n   ‚ö†Ô∏è  No topics detected (all below threshold {threshold})")
        
        print(f"\n   üìä All Topic Scores:")
        for topic, prob in zip(TOPICS, all_probs):
            emoji = "‚úÖ" if prob >= threshold else "‚ùå"
            print(f"      {emoji} {topic:12s}: {prob:.2%}")
    
    # Interactive mode
    if args.interactive:
        print("\n" + "=" * 80)
        print("üí¨ Interactive Mode (type 'quit' to exit)")
        print("=" * 80)
        
        while True:
            try:
                text = input("\nüìù Enter comment: ").strip()
                
                if text.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not text:
                    continue
                
                predictions, all_probs = predict(model, tokenizer, text, threshold, device)
                
                if predictions:
                    print(f"\n   ‚úÖ Predicted Topics:")
                    for pred in predictions:
                        print(f"      {pred['topic']:12s}: {pred['confidence']:.2%}")
                else:
                    print(f"\n   ‚ö†Ô∏è  No topics detected (all below threshold {threshold})")
                
                print(f"\n   üìä All Topic Scores:")
                for topic, prob in zip(TOPICS, all_probs):
                    emoji = "‚úÖ" if prob >= threshold else "‚ùå"
                    print(f"      {emoji} {topic:12s}: {prob:.2%}")
                    
            except KeyboardInterrupt:
                print("\n\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Test XLM-RoBERTa Topic Classifier')
    parser.add_argument(
        '--model_path',
        type=str,
        default='training/models/xlm_best_model.pt',
        help='Path to trained model checkpoint'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu'],
        help='Device to use for inference'
    )
    parser.add_argument(
        '--interactive',
        action='store_true',
        help='Enable interactive mode for manual testing'
    )
    
    args = parser.parse_args()
    main(args)
